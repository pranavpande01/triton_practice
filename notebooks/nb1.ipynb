{
          "cells": [
                    {
                              "cell_type": "code",
                              "execution_count": 1,
                              "id": "91f89965",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "#!pip install triton\n",
                                        "import torch\n",
                                        "import triton\n",
                                        "import triton.language as tl\n",
                                        "DEVICE = triton.runtime.driver.active.get_active_torch_device()\n",
                                        "  \n",
                                        "\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "9796fa5f",
                              "metadata": {},
                              "source": [
                                        "# GPU meaning: Many threads run together in lockstep groups on an SM.\n",
                                        "\n",
                                        "@triton.jit\n",
                                        "def add_kernel(x_ptr, n_elements, BLOCK: tl.constexpr):\n",
                                        "    pid = tl.program_id(0)\n",
                                        "    offsets = pid * BLOCK + tl.arange(0, BLOCK)\n",
                                        "    mask = offsets < n_elements\n",
                                        "    vals = tl.load(x_ptr + offsets, mask=mask, other=0.0)\n",
                                        "    tl.device_print('pid=', pid, ' first_val=', vals[0])\n",
                                        "\n",
                                        "size = 98432\n",
                                        "x = torch.rand(size, device=DEVICE)\n",
                                        "BLOCK = 128\n",
                                        "grid = (triton.cdiv(size, BLOCK),)\n",
                                        "add_kernel[grid](x, size, BLOCK=BLOCK)\n"
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "f05951cb",
                              "metadata": {},
                              "outputs": [
                                        {
                                                  "data": {
                                                            "text/plain": [
                                                                      "<triton.compiler.compiler.CompiledKernel at 0x7ef6c89eb110>"
                                                            ]
                                                  },
                                                  "execution_count": 7,
                                                  "metadata": {},
                                                  "output_type": "execute_result"
                                        }
                              ],
                              "source": [
                                        "# GPU meaning: Many threads run together in lockstep groups on an SM.\n",
                                        "size = 98432\n",
                                        "x_ptr = torch.rand(size, device=DEVICE)\n",
                                        "\n",
                                        "@triton.jit\n",
                                        "def add_kernel(x_ptr):\n",
                                        "\n",
                                        "    pid = tl.program_id(0)\n",
                                        "    offsets=pid * 128 + tl.arange(0, 128)\n",
                                        "    tl.device_print('pid=', offsets)\n",
                                        "    x=tl.load(x_ptr+offsets)\n",
                                        "    y=x*2\n",
                                        "    tl.store(y)\n",
                                        "                                 \n",
                                        "\n",
                                        "add_kernel[(4,)](x_ptr)\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "ba432ba1",
                              "metadata": {},
                              "source": [
                                        "\n",
                                        "triton.language.load(pointer, mask=None) -> Return a tensor of data whose values are loaded from memory at location defined by pointer.\n",
                                        "\n",
                                        "1. If pointer is a single element pointer, a scalar is be loaded.\n",
                                        "2. If pointer is an N-dimensional tensor of pointers, an N-dimensional tensor is loaded.\n",
                                        "3. If pointer is a block pointer defined by make_block_ptr, a tensor is loaded. \n",
                                        "\n",
                                        "\n"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "88fbb308",
                              "metadata": {},
                              "source": [
                                        "triton.language.store(pointer, value) -> Store a tensor of data into memory locations defined by pointer.\n",
                                        "\n",
                                        "1. If pointer is a single element pointer, a scalar is stored\n",
                                        "2. If pointer is an N-dimensional tensor of pointers, an N-dimensional block is stored. \n",
                                        "3. If pointer is a block pointer defined by make_block_ptr, a block of data is stored. "
                              ]
                    },
                    {
                              "cell_type": "code",
                              "execution_count": null,
                              "id": "a6f68508",
                              "metadata": {},
                              "outputs": [],
                              "source": [
                                        "@triton.jit\n",
                                        "def double(x_ptr,y_ptr):\n",
                                        "    pid=tl.program_id(0)\n",
                                        "    offsets=pid*128+tl.arange(0,128) # a list of pointers \n",
                                        "    x=tl.load(x_ptr+offsets)\n",
                                        "    y=2*x\n",
                                        "    tl.store(y_ptr+offsets,y)"
                              ]
                    },
                    {
                              "cell_type": "markdown",
                              "id": "aeabbcdf",
                              "metadata": {},
                              "source": [
                                        "Great question. The intent is:\n",
                                        "\n",
                                        "pid * BLOCK_SIZE gives the start index of the chunk this program should handle.\n",
                                        "\n",
                                        "pid = which program instance am I? (0, 1, 2, ...)\n",
                                        "BLOCK_SIZE = how many elements each program handles\n",
                                        "So each program gets a non-overlapping range.\n",
                                        "\n",
                                        "Example with BLOCK_SIZE = 8:\n",
                                        "\n",
                                        "pid=0 -> start = 0*8 = 0 -> handles indices 0..7\n",
                                        "pid=1 -> start = 1*8 = 8 -> handles indices 8..15\n",
                                        "pid=2 -> start = 2*8 = 16 -> handles indices 16..23\n"
                              ]
                    }
          ],
          "metadata": {
                    "kernelspec": {
                              "display_name": "Python 3 (ipykernel)",
                              "language": "python",
                              "name": "python3"
                    }
          },
          "nbformat": 4,
          "nbformat_minor": 5
}
